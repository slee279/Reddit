{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "69b9a648-bcc7-490d-9f9b-ea244d156bd6"
   },
   "source": [
    "# Using Reddit's API for Predicting Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-23T19:28:02.619411Z",
     "start_time": "2017-10-23T19:28:02.600856Z"
    }
   },
   "source": [
    "In this project, we will practice two major skills. Collecting data via an API request and then building a binary predictor.\n",
    "\n",
    "As we discussed in week 2, and earlier today, there are two components to starting a data science problem: the problem statement, and acquiring the data.\n",
    "\n",
    "For this article, your problem statement will be: _What characteristics of a post on Reddit contribute most to what subreddit it belongs to?_\n",
    "\n",
    "Your method for acquiring the data will be scraping threads from at least two subreddits. \n",
    "\n",
    "Once you've got the data, you will build a classification model that, using Natural Language Processing and any other relevant features, predicts which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a948d79c-5527-4c0d-ab23-f5d43ce72056"
   },
   "source": [
    "### Scraping Thread Info from Reddit.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a request (using requests) to the URL below. \n",
    "\n",
    "*NOTE*: Reddit will throw a [429 error](https://httpstatuses.com/429) when using the following code:\n",
    "```python\n",
    "res = requests.get(URL)\n",
    "```\n",
    "\n",
    "This is because Reddit has throttled python's default user agent. You'll need to set a custom `User-agent` to get your request to work.\n",
    "```python\n",
    "res = requests.get(URL, headers={'User-agent': 'YOUR NAME Bot 0.1'})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd \n",
    "import time\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a reddit api(json) url\n",
    "URL = \"http://www.reddit.com/r/boardgames.json\"\n",
    "\n",
    "# request api\n",
    "res = requests.get(URL, headers={'User-agent': 'SLEE279'})\n",
    "\n",
    "# check the status code. It should be 200\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `res.json()` to convert the response into a dictionary format and set this to a variable. \n",
    "\n",
    "```python\n",
    "data = res.json()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>approved_by</th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_background_color</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_richtext</th>\n",
       "      <th>author_flair_template_id</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author_flair_text_color</th>\n",
       "      <th>...</th>\n",
       "      <th>thumbnail_height</th>\n",
       "      <th>thumbnail_width</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>url</th>\n",
       "      <th>user_reports</th>\n",
       "      <th>view_count</th>\n",
       "      <th>visited</th>\n",
       "      <th>whitelist_status</th>\n",
       "      <th>wls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>#d0fffd</td>\n",
       "      <td></td>\n",
       "      <td>[{'e': 'text', 't': ''}]</td>\n",
       "      <td>326005ee-9cf2-11e8-8b1f-0e8f9a199476</td>\n",
       "      <td></td>\n",
       "      <td>dark</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>/r/boardgames Daily Discussion and Game Recomm...</td>\n",
       "      <td>8</td>\n",
       "      <td>https://www.reddit.com/r/boardgames/comments/9...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>rhaffos</td>\n",
       "      <td></td>\n",
       "      <td>patchwork</td>\n",
       "      <td>[{'e': 'text', 't': 'Patchwork'}]</td>\n",
       "      <td>58e32966-18ab-11e7-b45f-0ed78322c06a</td>\n",
       "      <td>Patchwork</td>\n",
       "      <td>dark</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3D Printed Sushi Go Party! Insert</td>\n",
       "      <td>135</td>\n",
       "      <td>https://www.reddit.com/r/boardgames/comments/9...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>all_ads</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  approved_at_utc approved_by  archived         author  \\\n",
       "0            None        None     False  AutoModerator   \n",
       "1            None        None     False        rhaffos   \n",
       "\n",
       "  author_flair_background_color author_flair_css_class  \\\n",
       "0                       #d0fffd                          \n",
       "1                                            patchwork   \n",
       "\n",
       "               author_flair_richtext              author_flair_template_id  \\\n",
       "0          [{'e': 'text', 't': ''}]  326005ee-9cf2-11e8-8b1f-0e8f9a199476   \n",
       "1  [{'e': 'text', 't': 'Patchwork'}]  58e32966-18ab-11e7-b45f-0ed78322c06a   \n",
       "\n",
       "  author_flair_text author_flair_text_color ...  thumbnail_height  \\\n",
       "0                                     dark ...               NaN   \n",
       "1         Patchwork                    dark ...               NaN   \n",
       "\n",
       "  thumbnail_width                                              title  ups  \\\n",
       "0             NaN  /r/boardgames Daily Discussion and Game Recomm...    8   \n",
       "1             NaN                  3D Printed Sushi Go Party! Insert  135   \n",
       "\n",
       "                                                 url  user_reports view_count  \\\n",
       "0  https://www.reddit.com/r/boardgames/comments/9...            []       None   \n",
       "1  https://www.reddit.com/r/boardgames/comments/9...            []       None   \n",
       "\n",
       "   visited whitelist_status  wls  \n",
       "0    False          all_ads    6  \n",
       "1    False          all_ads    6  \n",
       "\n",
       "[2 rows x 95 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the response into a dictionary format and set that to 'data' variable\n",
    "data = res.json()\n",
    "\n",
    "# just observing data, and getting used to the format\n",
    "# check the dictionary\n",
    "data\n",
    "\n",
    "# check dictionary keys\n",
    "data.keys()\n",
    "\n",
    "# what's in 'data'\n",
    "data['data']\n",
    "\n",
    "# what are 'data's keys\n",
    "data['data'].keys()\n",
    "\n",
    "# post for a particular page\n",
    "data['data']['children']\n",
    "\n",
    "# number of subreddits in the page\n",
    "len(data['data']['children']) \n",
    "\n",
    "# 2nd item of the list\n",
    "data['data']['children'][3] \n",
    "\n",
    "# checking dictionary keys\n",
    "data['data']['children'][0].keys()\n",
    "\n",
    "# what's in the data?\n",
    "data['data']['children'][0]['data'] \n",
    "\n",
    "# label\n",
    "data['data']['children'][0]['data']['subreddit'] \n",
    "\n",
    "# title\n",
    "data['data']['children'][0]['data']['title'] \n",
    "\n",
    "# description\n",
    "data['data']['children'][0]['data']['selftext'] \n",
    "\n",
    "# again, keys\n",
    "data['data']['children'][0]['data'].keys() \n",
    "\n",
    "# all the posts\n",
    "posts = [post['data'] for post in data['data']['children']]\n",
    "\n",
    "# to check dictionary shape and compare to data\n",
    "temp = pd.DataFrame(posts) \n",
    "\n",
    "# len(posts) should be same as row of temp.shape\n",
    "len(posts)\n",
    "\n",
    "# check\n",
    "temp.shape\n",
    "\n",
    "temp.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting more results\n",
    "\n",
    "By default, Reddit will give you the top 25 posts:\n",
    "\n",
    "```python\n",
    "print(len(data['data']['children']))\n",
    "```\n",
    "\n",
    "If you want more, you'll need to do two things:\n",
    "1. Get the name of the last post: `data['data']['after']`\n",
    "2. Use that name to hit the following url: `http://www.reddit.com/r/boardgames.json?after=THE_AFTER_FROM_STEP_1`\n",
    "3. Create a loop to repeat steps 1 and 2 until you have a sufficient number of posts. \n",
    "\n",
    "*NOTE*: Reddit will limit the number of requests per second you're allowed to make. When you create your loop, be sure to add the following after each iteration.\n",
    "\n",
    "```python\n",
    "time.sleep(3) # sleeps 3 seconds before continuing```\n",
    "\n",
    "This will throttle your loop and keep you within Reddit's guidelines. You'll need to import the `time` library for this to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.reddit.com/r/boardgames.json?after=t3_9mhwm6'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name of the last post\n",
    "data['data']['after']\n",
    "\n",
    "# new url being updated\n",
    "URL + '?after=' + data['data']['after'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves subreddit dataframe into csv files\n",
    "def scrape(subreddit, times):\n",
    "    \"\"\"\n",
    "    Saves subreddit dataframe into a csv file as (subreddit name).csv\n",
    "    \n",
    "    Parameters:\n",
    "    subreddit: name of the subreddit to scrape\n",
    "    times: number of pages you want to scrape\n",
    "    \n",
    "    return: print \"Finished\" when done\n",
    "    \"\"\"\n",
    "    \n",
    "    posts = []\n",
    "    after = None\n",
    "    done = 'Finished'\n",
    " \n",
    "    # load subreddit.json\n",
    "    URL = 'http://www.reddit.com/r/' + subreddit + '.json'\n",
    "    \n",
    "    for times in range(times):\n",
    "        if after == None:\n",
    "            current_url = URL\n",
    "        else:\n",
    "            current_url = URL + '?after=' + after\n",
    "        \n",
    "        # print pages are being scraped\n",
    "        # after 5 pages it will print just multiples of 5 pages. (5 is minimum pages showing)\n",
    "        if times < 4:\n",
    "            print('URL', str(times+1), current_url)\n",
    "            \n",
    "        if (times + 1) % 5 == 0:\n",
    "            print('URL', str(times+1), current_url)\n",
    "\n",
    "        res = requests.get(current_url, headers={'User-agent': 'SLEE279'})\n",
    "        if res.status_code != 200:\n",
    "            print('Status not 200', res.status_code)\n",
    "            break\n",
    "\n",
    "        current_dict = res.json()\n",
    "        current_posts = [post['data'] for post in current_dict['data']['children']]\n",
    "        posts.extend(current_posts)\n",
    "        after = current_dict['data']['after']\n",
    "        time.sleep(3)\n",
    "\n",
    "        pd.DataFrame(posts).to_csv((subreddit + '.csv'), index=False)\n",
    "        \n",
    "    return done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "43e71edd-210e-42b1-9336-70a931f048af"
   },
   "source": [
    "### Save your results as a CSV\n",
    "You may do this regularly while scraping data as well, so that if your scraper stops of your computer crashes, you don't lose all your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL 1 http://www.reddit.com/r/mlb.json\n",
      "URL 2 http://www.reddit.com/r/mlb.json?after=t3_9mkd10\n",
      "URL 3 http://www.reddit.com/r/mlb.json?after=t3_9lpqr8\n",
      "URL 4 http://www.reddit.com/r/mlb.json?after=t3_9lc3ip\n",
      "URL 5 http://www.reddit.com/r/mlb.json?after=t3_9kxbr5\n",
      "URL 10 http://www.reddit.com/r/mlb.json?after=t3_9iml8q\n",
      "URL 15 http://www.reddit.com/r/mlb.json?after=t3_9dwk89\n",
      "URL 20 http://www.reddit.com/r/mlb.json?after=t3_99cx2o\n",
      "URL 25 http://www.reddit.com/r/mlb.json?after=t3_95unwb\n",
      "URL 30 http://www.reddit.com/r/mlb.json?after=t3_938wi0\n"
     ]
    }
   ],
   "source": [
    "# Data Frames are made inside the function and saved to a csv file.\n",
    "# scrape 50 pages from each of subreddits\n",
    "# put two subreddits name into list 'subreddit'\n",
    "\n",
    "subreddit = ['mlb', 'nba']\n",
    "for name in subreddit:\n",
    "    scrape(name, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "# converts csv file into a cleaned dataframe\n",
    "def data_clean(csv_file):\n",
    "    \"\"\"\n",
    "    Cleans data frame from scraping then converts to a cleaned dataframe\n",
    "    Removes where title or selftext is NaN\n",
    "    Removes articles from Mod\n",
    "    \n",
    "    Create a new data frame with a row 'selftext'(Description), 'title'(Title), and 'target'(Target, y = name of the subreddit)\n",
    "    \n",
    "    Parameters:\n",
    "    csv_file: scraped csv file from a subreddit\n",
    "    \"\"\"\n",
    "    cleaned = pd.DataFrame(columns=('Title', 'Description', 'Target'))\n",
    "    data = pd.read_csv(csv_file)\n",
    "    \n",
    "    data = data[(data['title'].notnull()) & (data['selftext'].notnull()) & (data['author'] != 'AutoModerator')] \n",
    "    \n",
    "    cleaned['Title'] = data['title']\n",
    "    cleaned['Description'] = data['selftext']\n",
    "    \n",
    "    # removes '.csv' for naming\n",
    "    cleaned['Target'] = csv_file[:-4]\n",
    "\n",
    "    # reset the index\n",
    "    cleaned.reset_index(drop=True, inplace=True) \n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# read a csv file then convert to pd.Dataframe\n",
    "# clean the data\n",
    "# there are only two subreddits to scrape\n",
    "for i in range(2):\n",
    "    subreddit[i] = data_clean(subreddit[i] + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "# now subreddit is a list of dataframes\n",
    "subreddit[0].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all dataframes into one data frame\n",
    "full_data = pd.concat([subreddit[0], subreddit[1]], ignore_index=True)\n",
    "full_data.head()\n",
    "full_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "db045898-1d2d-4af2-8e79-437c4c7546b4"
   },
   "source": [
    "## NLP\n",
    "\n",
    "#### Use `CountVectorizer` or `TfidfVectorizer` from scikit-learn to create features from the thread titles and descriptions (NOTE: Not all threads have a description)\n",
    "- Examine using count or binary features in the model\n",
    "- Re-evaluate your models using these. Does this improve the model performance? \n",
    "- What text features are the most valuable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = full_data['Description']\n",
    "y = full_data['Target']\n",
    "\n",
    "# custom stopwords\n",
    "custom_stopwords = list(ENGLISH_STOP_WORDS) + ['www', 'know', 'https', 'just', 'like', 'com', 'do', 'does']\n",
    "tv = TfidfVectorizer(stop_words=custom_stopwords)\n",
    "X_tv = tv.fit_transform(X)\n",
    "X_tv_df = pd.DataFrame(X_tv.toarray(), columns=tv.get_feature_names())\n",
    "X_tv_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_tv_df.sum().sort_values(ascending=False).head(10).plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "04563b69-f7b6-466f-9d65-fc62c9ddee6a"
   },
   "source": [
    "## Predicting subreddit using Random Forests + Another Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "c7631f51-07f2-4c79-a093-3e9bc7849a48"
   },
   "source": [
    "#### We want to predict a binary variable - class `0` for one of your subreddits and `1` for the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "c20d2498-151c-44c3-a453-3a333c79a0ac"
   },
   "outputs": [],
   "source": [
    "# 1 if the first target else 0\n",
    "y_bin = [1 if i == str(subreddit[0].Target[0]) else 0 for i in full_data.Target]\n",
    "# y_bin.count(0)\n",
    "# subreddit[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a `RandomForestClassifier` model to predict which subreddit a given post belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, split data\n",
    "# put y_bin instead of y, since we want 1 or 0\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_bin, test_size=0.3, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform our train and test data (X)\n",
    "X_tr_tv = tv.fit_transform(X_train)\n",
    "X_te_tv = tv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "588f9845-6143-4bcc-bfd1-85d45b79303d"
   },
   "outputs": [],
   "source": [
    "# randomforest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_tr_tv, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# random forest score on the test set\n",
    "rf.score(X_te_tv, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = rf.predict(X_te_tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "a7afb2c0-d41e-4779-8216-91cd8dd4473f"
   },
   "source": [
    "#### Thought experiment: What is the baseline accuracy for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline accuracy, the percentage of higher(majority)\n",
    "ba = full_data['Target'].value_counts(normalize=True)[0] * 100 # show percentage\n",
    "ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "87a17d3d-b7f4-4747-9f75-f9af1d18a174"
   },
   "outputs": [],
   "source": [
    "# a confusion matrix \n",
    "\n",
    "def make_pretty_conmat(y_test, predictions):\n",
    "    conmat = confusion_matrix(y_test, predictions)\n",
    "    conmat_df = pd.DataFrame(conmat)\n",
    "    conmat_df = conmat_df.add_prefix('Predicted: ')\n",
    "    print(f\"accuracy score: {accuracy_score(y_test, predictions)}\")\n",
    "    return conmat_df\n",
    "\n",
    "make_pretty_conmat(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "9367beff-72ba-4768-a0ba-a50b335de61d"
   },
   "source": [
    "#### Use cross-validation in scikit-learn to evaluate the model above. \n",
    "- Evaluate the accuracy of the model, as well as any other metrics you feel are appropriate. \n",
    "- **Bonus**: Use `GridSearchCV` with `Pipeline` to optimize your `CountVectorizer`/`TfidfVectorizer` and classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "focus": false,
    "id": "269b9e7c-60b5-4a06-8255-881d7395bc1b"
   },
   "outputs": [],
   "source": [
    "# using GridSearchCV with Pipeline to optimize TfidfVectorizer with Randomforest\n",
    "pipeline = Pipeline([\n",
    "    ('tv', tv),\n",
    "    ('rf', rf)\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'tv__stop_words': [None, custom_stopwords, ENGLISH_STOP_WORDS],\n",
    "    'tv__max_features': [3000, 4000],\n",
    "    'tv__ngram_range': [(1, 1), (1, 2)],\n",
    "    'rf__n_estimators': [50, 100, 150]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipeline, param_grid=params, cv=5)\n",
    "gs.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV score on test set\n",
    "gsscore = gs.score(X_test, y_test)\n",
    "gsscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best parameters of GridSearchCV using pipeline\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the model-building process using a different classifier (e.g. `MultinomialNB`, `LogisticRegression`, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_tr_tv, y_train)\n",
    "lrscore = lr.score(X_te_tv, y_test)\n",
    "\n",
    "# MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_tr_tv, y_train)\n",
    "nbscore = nb.score(X_te_tv, y_test)\n",
    "\n",
    "# GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "gb.fit(X_tr_tv, y_train)\n",
    "gbscore = gb.score(X_te_tv, y_test)\n",
    "\n",
    "# RandomForestClassifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_tr_tv, y_train)\n",
    "rfscore = rf.score(X_te_tv, y_test)\n",
    "\n",
    "# create a table\n",
    "table = pd.DataFrame(index=['Logistic Regression', 'Multinomial NB', 'Gradient Boosting', 'Random Forest', 'GridSearchCV'])\n",
    "table.index.name = 'Model'\n",
    "table['Test Data Score'] = [lrscore, nbscore, gbscore, rfscore, gsscore]\n",
    "\n",
    "table.sort_values('Test Data Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Analyze the coefs \n",
    "\n",
    "coefs = pd.DataFrame(lr.coef_,columns=tv.get_feature_names()).T\n",
    "coefs['ABS'] = coefs[0].abs()\n",
    "coefs.sort_values(by='ABS', ascending=False).head(20)[0].plot(kind='barh', cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "---\n",
    "Put your executive summary in a Markdown cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nike is the one of the biggest sportswear company in the world. The challenge is that Nike is sharing their customers' interests such as Adidas and Under Armor. One of the methods that attract customers to buy Nike products is putting ads where people see most. So, where should Nike put ads?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding places where you put ads is not easy. Where are people that are interested in buying Nike products? Which website/subreddit should you pick to get more attention from people who are interested in purchasing new sports products? I am here for you to find the best places online. Putting ads in the right places will increase the number of buyers and will help to beat your competitors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My model analyzes words in an article then find out which subreddit (website) it came from. It will also show the frequency of the words, so the user can grasp what people are talking about most. By understanding what people are talking about in a specific thread, Nike will be able to put a related advertisement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, using this tool, Nike will find a subreddit which people talk a lot about LeBron James where the company can put new LeBron James basketball shoes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I love working with Nike that repects diversity. I want to contribute on Nike's success in near future. In order to increase the number of people who 'Just Buy It', I recommend using this model to analyze customers' needs and their thoughts. What are your buyers talking now? Which subreddit they talk most? Let's figure things out.\n",
    "Just Do It with my model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
